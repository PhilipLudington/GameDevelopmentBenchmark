name: Benchmark

on:
  workflow_dispatch:
    inputs:
      models:
        description: "Comma-separated list of models to evaluate"
        required: true
        default: "mock:pass"
      category:
        description: "Task category filter (leave empty for all)"
        required: false
        default: ""
      tier:
        description: "Task tier filter (1-4, leave empty for all)"
        required: false
        default: ""

  schedule:
    # Run weekly on Sundays at midnight UTC
    - cron: "0 0 * * 0"

jobs:
  benchmark:
    name: Run Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libsdl2-2.0-0 libsdl2-mixer-2.0-0 libsdl2-image-2.0-0

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set headless mode
        run: |
          echo "SDL_VIDEODRIVER=dummy" >> $GITHUB_ENV
          echo "SDL_AUDIODRIVER=dummy" >> $GITHUB_ENV

      - name: Parse models
        id: parse_models
        run: |
          # Convert comma-separated models to -m flags
          MODELS="${{ github.event.inputs.models || 'mock:pass' }}"
          MODEL_FLAGS=""
          IFS=',' read -ra MODEL_ARRAY <<< "$MODELS"
          for model in "${MODEL_ARRAY[@]}"; do
            MODEL_FLAGS="$MODEL_FLAGS -m $model"
          done
          echo "flags=$MODEL_FLAGS" >> $GITHUB_OUTPUT

      - name: Run benchmark
        run: |
          CATEGORY_FLAG=""
          if [ -n "${{ github.event.inputs.category }}" ]; then
            CATEGORY_FLAG="-c ${{ github.event.inputs.category }}"
          fi

          TIER_FLAG=""
          if [ -n "${{ github.event.inputs.tier }}" ]; then
            TIER_FLAG="--tier ${{ github.event.inputs.tier }}"
          fi

          python scripts/run_benchmark.py \
            ${{ steps.parse_models.outputs.flags }} \
            $CATEGORY_FLAG \
            $TIER_FLAG \
            --output results/runs \
            --report

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: results/runs/
          retention-days: 90

      - name: Upload HTML report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ github.run_id }}
          path: results/runs/*/report.html
          retention-days: 90

  update-leaderboard:
    name: Update Leaderboard
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v4

      - name: Download results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: results/runs/

      - name: Find latest report
        id: find_report
        run: |
          LATEST=$(ls -td results/runs/*/ | head -1)
          echo "latest=$LATEST" >> $GITHUB_OUTPUT

      - name: Copy to leaderboard
        run: |
          cp "${{ steps.find_report.outputs.latest }}/report.html" leaderboard/index.html || true
          cp "${{ steps.find_report.outputs.latest }}/report.json" leaderboard/latest.json || true

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        if: success()
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./leaderboard
          publish_branch: gh-pages
